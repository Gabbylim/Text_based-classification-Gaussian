#text preprocessing
import re,string
import nltk
import nltk.tokenize import wrd_tokenize
from nltk.corpus import stopwords
from nltk.corpus import wordnet
fromnltk.stem import WordNetLemmatizer
from nltk.stem import SnowballStemmer
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')

def preprocess(text):


text = text.lower()
text = re.compile("[(+*$!@%&#)]*").sub('',text).strip()
text = re.sub('\s+', ' ', text)
text = re.sub(r'\[[0-9]*\]',' ',text)
text=re.sub(r'[^\w\s]', '', str(text).lower().strip())
text = re.sub(r'\d',' ',text) 
text = re.sub(r'\s+',' ',text)
return text


def stopword(string):

 a = [i for i in string.split() if i is not in stopwords.words('english')]
return ''.join(a)



def lemmatize(string):
 a = []
 lem = WordNetLemmatizer()
 a = [lem.lemmatize(string)]
 return a

def finalpreprocess(string):
  return lemmatizer(stopword(preprocess(string))
  
